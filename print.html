<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Research paper summaries</title>
        <meta name="robots" content="noindex" />
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="./theme/catppuccin.css">
        <link rel="stylesheet" href="./theme/catppuccin-highlight.css">
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="index.html">Introduction</a></li><li class="chapter-item expanded "><a href="deep-residual-learning-for-image-recognition.html"><strong aria-hidden="true">1.</strong> Deep residual learning for image recognition</a></li><li class="chapter-item expanded "><a href="yolo.html"><strong aria-hidden="true">2.</strong> YOLO</a></li><li class="chapter-item expanded "><a href="world-models.html"><strong aria-hidden="true">3.</strong> World Models</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="latte">Latte</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="frappe">Frappé</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="macchiato">Macchiato</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mocha">Mocha</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Research paper summaries</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="about"><a class="header" href="#about">About</a></h1>
<p>A repository for short summaries on ML papers that I've read.</p>
<h2 id="2015"><a class="header" href="#2015">2015</a></h2>
<ul>
<li><code>Object Detection</code> <a href="yolo.html">You Only Look Once: Unified, Real-Time Object Detection (aka YOLO)</a></li>
<li><code>Network Architecture</code> <a href="deep-residual-learning-for-image-recognition.html">Deep Residual Learning for Image Recognition</a></li>
</ul>
<h2 id="2018"><a class="header" href="#2018">2018</a></h2>
<ul>
<li><code>Reinforcement Learning</code> <a href="world-models.html">World Models</a></li>
</ul>
<br>
<p>A paper on <a href="https://web.stanford.edu/class/ee384m/Handouts/HowtoReadPaper.pdf">How to Read a Paper</a>
<br></p>
<p>Inspired by <a href="https://github.com/aleju/papers">aleju/papers</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="paper"><a class="header" href="#paper">Paper</a></h1>
<ul>
<li><strong>Title</strong>: Deep Residual Learning for Image Recognition</li>
<li><strong>Authors</strong>: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun</li>
<li><strong>Link</strong>: <a href="https://arxiv.org/abs/1512.03385">https://arxiv.org/abs/1512.03385</a></li>
<li><strong>Year</strong>: 2015</li>
</ul>
<h2 id="what"><a class="header" href="#what">What</a></h2>
<!-- * This degradation is not caused by overfitting but rather by adding more
layers which make the model hard to optimize. -->
<ul>
<li>They propose a residual learning framework which eases the training of deep networks.</li>
<li>ResNet 8x deeper than VGGNet, with 152 layers, has significantly less parameters and performs
better in ImageNet dataset.</li>
<li>Ensemble of ResNets achieve 3.57% error on ImageNet test dataset.</li>
<li>Deep networks learn more enriched low/mid/high level features as we increase
the depth of the network.</li>
<li>Recent evidence reveals that this is not the case as the training error
increases as we increase the layers.</li>
<li>This degradation is not caused by overfitting but rather by deep networks
which are hard to optimize.</li>
<li>They argue that deeper models should produce error rates no higher than their
shallower counterpart as the
smaller networks' solution space is a subspace of the original networks'
solution space and the stacked layers
should at least learn the identity mappings.</li>
<li>So instead of hoping that the network would learn the identity mappings they
explicitly reformulate the
original function to <code>F(x) + x</code>(skip connections), where <code>x</code> is the output of
one of the previous layers.</li>
<li>They hypothesize that it is easier to optimize the residual mapping.</li>
</ul>
<br>
<p align="center">
<img src="images/deep-residual-learning-for-image-recognition/performance-degradation.png"
alt="Performance Degradation" width=450>
<img src="images/deep-residual-learning-for-image-recognition/resnet-block.png"
alt="Residual Block" width=300>
</p>
<br>
<h2 id="how"><a class="header" href="#how">How</a></h2>
<ul>
<li>
<p>Model Overview</p>
<ul>
<li>They reformulate the original function <code>H(x) = F(x)</code> to <code>H(x) = F(x) + x</code> as hypothesized.</li>
<li>If the identity mappings are optimal then the network can just drive the layer weights to 0.</li>
<li>Skip connections can be used in both fully connected and convolutional layers.</li>
</ul>
</li>
</ul>
<br>
<p align="center">
<img src="images/deep-residual-learning-for-image-recognition/basic-resblock.png" alt="Residual Blocks">
</p>
<ul>
<li>
<p>Model Architecture</p>
<ul>
<li>Skip connections are made b/w 2 or 3 layers.</li>
<li>They don't see any improvements for 1 layer skip connections, i.e <code>(W * x) + x</code>.</li>
<li>If the dimensions of the <code>F(x)</code> and <code>x</code> don't match, then they propose a few methods to match them:
<ol>
<li>Add zero padded entries for the extra dimensions(no extra parameters).</li>
<li>Use <code>1x1</code> convolutions with matching dimensions(extra parameters).</li>
</ol>
</li>
<li>Batch Normalization is used right after convolution and before activation function.</li>
<li>ImageNet model specifics:
<ul>
<li>Learning rate = 0.1 and is divided by 10 when the error plateaus.</li>
<li>Weight decay of 0.0001 and a momentum of 0.9.</li>
<li>He initialization is used.</li>
<li>Mini-batch of size 256.</li>
</ul>
</li>
</ul>
</li>
</ul>
<br>
<p align="center">
<img src="images/deep-residual-learning-for-image-recognition/model-diagram.png" alt="Architecture">
<img src="images/deep-residual-learning-for-image-recognition/model-complexity.png" alt="Complexity">
</p>
<ul>
<li>
<p>Training/Experiments</p>
<ul>
<li>Models are trained on the 1.28 million training images from ImageNet which has 1000 classes.</li>
<li>They train a bunch of models with 18, 34, 50, 101, 152 layers.</li>
<li>They experiment 3 different ways to match dimensions b/w residual layers: 1. (A) Zero-padding shortcuts. 2. (B) Projection shortcuts(1x1 convs) are used for increasing dimensions. 3. (C) All shortcuts are projections.
<br>
out of which (B) works the best.</li>
<li>They also explore &gt; 1000 layer network which performs worse than their
110 layer network, they argue that this is because of overfitting as both
models have same training error.</li>
</ul>
</li>
</ul>
<br>
<p align="center">
<img
src="images/deep-residual-learning-for-image-recognition/training-graph.png"
alt="Training graph">
</p>
<ul>
<li>
<p>Results</p>
<ul>
<li>They have no problem optimizing very deep networks(1000 layers).</li>
<li>Activation signals are low compared to other &quot;plain&quot; networks.</li>
<li>They also generalize well on other recognition tasks such as object detection.</li>
<li>The plain networks error rate goes up from 27.94% to 28.54% as the number
of layers increased from 18 to 34 but in the case of ResNets it decreases
from 27.88% to 25.03% for the same.</li>
</ul>
</li>
</ul>
<p align="center">
<img
src="images/deep-residual-learning-for-image-recognition/training-results.png"
alt="Training graph">
</p>
<h3 id="related-material"><a class="header" href="#related-material">Related Material</a></h3>
<ul>
<li>Crisp and compact explanation - <a href="https://youtu.be/sAzL4XMke80">ResNet Explained! - Henry AI Labs</a></li>
<li>Detailed walkthrough - <a href="https://youtu.be/GWt6Fu05voI">Deep Residual Learning for Image Recognition (Paper Explained) - Yannic Kilcher</a></li>
<li><a href="https://www.reddit.com/r/MachineLearning/comments/67gonq/d_batch_normalization_before_or_after_relu">The age old argument on where to BN</a></li>
<li><a href="https://www.quora.com/Is-there-a-theory-for-why-batch-normalization-has-a-regularizing-effect">Ian Goodfellow on why BN works</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="paper-1"><a class="header" href="#paper-1">Paper</a></h1>
<ul>
<li><strong>Title</strong>: You Only Look Once: Unified, Real-Time Object Detection</li>
<li><strong>Authors</strong>: Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi</li>
<li><strong>Link</strong>: <a href="https://arxiv.org/abs/1506.02640">https://arxiv.org/abs/1506.02640</a></li>
<li><strong>Code</strong>: <a href="https://github.com/pjreddie/darknet">pjreddie/darknet</a></li>
<li><strong>Year</strong>: 2015</li>
</ul>
<h2 id="what-1"><a class="header" href="#what-1">What</a></h2>
<ul>
<li>The proposed model predicts bounding boxes on images.</li>
<li>Reframes classification as a regression problem.</li>
<li>Makes localization errors but faster than R-CNN, processing images at 155 FPS.</li>
<li>End-to-end training, a single CNN simultaneously predicts the multiple bounding boxes and class
scores for those boxes.</li>
<li>Reasons globally about the image as it trains on the entire image rather than using methods like
sliding window
which trains on local regions.</li>
</ul>
<h2 id="how-1"><a class="header" href="#how-1">How</a></h2>
<ul>
<li>
<p>Model Overview</p>
<ul>
<li>Divides image in to a <code>SxS</code> grid.</li>
<li>Each grid cell predicts <code>B</code> bounding boxes.</li>
<li>Each bounding box predicts 5 values - x, y, w, h and confidence score.
<ul>
<li>(x, y) coordinates represent the center of the box relative to the grid cell.</li>
<li>w, h represent the width and height of the box relative to the whole image.</li>
<li>Confidence scores are equal to zero if there is no object in the grid cell and is
calculated as the IOU
(Intersection over Union) b/w the predicted box and the ground truth otherwise.</li>
</ul>
</li>
<li>Class probabilities to decide which of the <code>C</code> classes (<code>C</code> = number of labelled classes in
the dataset) the object belongs to.</li>
</ul>
</li>
</ul>
<br>
<p align="center">
<img src="images/yolo/overview.png" alt="Overview">
<img src="images/yolo/iou.jpeg" alt="IOU">
</p>
<br>
<ul>
<li>
<p>Model Architecture</p>
<ul>
<li>Input images are of size <code>448x448x3</code>.</li>
<li>Output are <code>S x S x (B*5 + C)</code> values.
<ul>
<li><code>S</code> is the grid size, default = 7.</li>
<li><code>B</code> is the number of predicted bounding boxes, default = 2.</li>
<li><code>C</code> is the number of labelled classes in the dataset.</li>
</ul>
</li>
<li>For each <code>B</code> bounding box it predicts 5 values: <code>x</code>, <code>y</code>, <code>w</code>, <code>h</code> and <code>confidence</code>, where:
<ul>
<li>(<code>x</code>, <code>y</code>) represent the center of the box relative to the grid cell.</li>
<li>(<code>w</code>, <code>h</code>) represent the width and height relative to the image.</li>
<li><code>confidence</code> represents the IOU b/w the predicted and ground truth box.</li>
</ul>
</li>
<li>Predicts <code>B</code> bounding boxes per grid cell and chooses the one with highest IOU during training
essentially learning to predict certain objects, aspect ratios and improve overall recall.</li>
<li>They use 24 convolutional layers followed by 2 fully connected layers with some having 1x1 convs and few having 3x3 convs.</li>
<li>The Fast YOLO uses 9 conv layers and fewer filters, all other parameters are the same for those.</li>
<li>Use linear activation function for the final layer and leaky RELUs with alpha = <code>0.1</code> for the rest.</li>
<li>Use dropout with <code>0.5</code> after the first layer to prevent co-adaptation b/w layers.</li>
<li>Use momentum of <code>0.9</code> and decay of <code>0.0005</code> - pretty standard stuff.</li>
<li>Gradually increase learning rate from 10<sup>-3</sup> to 10<sup>-2</sup> for the first epoch
and continue training with that till 75 epochs, then 10<sup>-3</sup> for next 30 epochs and
finally 10<sup>-4</sup> for the rest.</li>
<li>They found that if they started with a high learning rate then the model often diverges due to unstable gradients.</li>
</ul>
</li>
</ul>
<br>
<p align="center">
<img src="images/yolo/architecture.png" alt="Architecture">
</p>
<br>
<ul>
<li>
<p>Training</p>
<ul>
<li>They pre-train the model on ImageNet and then finetune on the Pascal VOC dataset.</li>
<li>Optimize over the sum-squared error</li>
<li>Since most grid cells do not have objects, these gradients overpower the gradients from cells
with objects in them leading to model instability. To account for this, they weigh the
bounding box loss higher than the loss from confidence using 2 parameters λ<sub>coord</sub> =
5 for bounding boxes and λ<sub>noobj</sub> = 0.5 for confidence.</li>
<li>Sum-squared error equally weighs errors in large and small boxes. Small deviations in large
bounding boxes would have a small effect than in small bounding boxes, to address this issue
they predict the square root of the width and height resulting in similar losses for both
large and small bounding boxes.</li>
</ul>
<br>
<p align="center">
<img src="images/yolo/loss-func.png" alt="Loss Function">
</p>
<br>
</li>
<li>
<p>Results</p>
<ul>
<li>Struggles with too many small nearby objects, such as flock of birds.</li>
<li>Struggles to generalize bounding box on new unusual aspect ratio images.</li>
<li>YOLO scores a 57.9 mAP compared to 70.4 with the Faster R-CNN on the Pascal VOC 2012 dataset.</li>
<li>Pascal VOC 2012 Leaderboard
<img src="images/yolo/leaderboard.png" alt="Leaderboard"></li>
<li>They achieve 45 FPS (22ms/image), compared to 7 FPS (142ms/image) with Faster R-CNN.</li>
<li>YOLO on a diverse dataset, as you can see it generalizes well on artworks too.
<img src="images/yolo/results.png" alt="Results"></li>
</ul>
</li>
</ul>
<h3 id="related-material-1"><a class="header" href="#related-material-1">Related Material</a></h3>
<ul>
<li><a href="https://pjreddie.com/darknet/yolo">pjreddie.com/yolo</a></li>
<li><a href="https://github.com/AlexeyAB/darknet">AlexeyAB/darknet</a></li>
<li><a href="https://github.com/aleju/papers/blob/master/neural-nets/YOLO.md">aleju/papers/YOLO.md</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="paper-2"><a class="header" href="#paper-2">Paper</a></h1>
<ul>
<li><strong>Title</strong>: World Models</li>
<li><strong>Authors</strong>: David Ha, Jurgen Schmidhuber</li>
<li><strong>Link</strong>: <a href="https://arxiv.org/abs/1803.10122.pdf">https://arxiv.org/abs/1803.10122.pdf</a></li>
<li><strong>Year</strong>: 2018</li>
</ul>
<h2 id="what-2"><a class="header" href="#what-2">What</a></h2>
<ul>
<li>World model can be used to learn a compressed spatial and temporal representation of the
environment.</li>
<li>Train on &quot;hallucinated dreams&quot; and transfer the model back to the original environment.</li>
</ul>
<h2 id="how-2"><a class="header" href="#how-2">How</a></h2>
<ul>
<li>
<h3 id="overviewmotivation"><a class="header" href="#overviewmotivation">Overview/Motivation</a></h3>
<ul>
<li>We humans develop an internal model of the world.</li>
<li>We only learn concepts and relationships and use them to represent the real world.</li>
<li>Our brain learns an abstract representation of both spatial and temporal aspects of
information.</li>
<li>First train the world model in unsupervised manner and train the small controller model based
off of the world model to act in the env (Note that the world model does all the heavy
lifting and the controller just needs to act in the latent space).</li>
</ul>
</li>
</ul>
<br>
<p align="center">
<img src="images/world-models/model-working.png"
alt="Performance Degradation" width=450>
</p>
<br>
<ul>
<li>
<h3 id="agent-model"><a class="header" href="#agent-model">Agent Model</a></h3>
<ol>
<li>Visual sensory data (VAE)</li>
<li>Memory component (RNN + MDN)</li>
<li>Decision making component (Controller)</li>
</ol>
<ul>
<li>
<h4 id="vae-v"><a class="header" href="#vae-v">VAE (V)</a></h4>
<ul>
<li>The job of the VAE is to generate an efficient latent space of the original frame.<!-- TODO -->
<!-- * They mention that the image is 2D, does that mean that the model is trained on B/W images -->
<!--   instead of color images? -->
</li>
<li>Predicts <code>z(t)</code> where <code>z</code> is the latent space representation and <code>t</code> is the timestep.</li>
</ul>
</li>
<li>
<h4 id="rnn--mdnm"><a class="header" href="#rnn--mdnm">RNN + MDN(M)</a></h4>
<ul>
<li>
<p>This model compresses what happens over time. so for this purpose it predicts the future.
serves as a predictive model of the future z vectors that V is expected to produce.</p>
</li>
<li>
<p>Predicts <code>P(z(t+1) | a(t), z(t), h(t))</code>, where <code>z(t)</code> is the latent space representation
of the current observation, <code>a(t)</code> is the action taken at timestep <code>t</code>, <code>h(t)</code> is the
hidden state of the RNN at time <code>t</code>.</p>
</li>
<li>
<p>Note that it predicts <code>P(z(t+1))</code> instead of a deterministic <code>z(t+1)</code>. This is because
most environments are stochastic in nature so it outputs a Probability Density
Function(PDF).</p>
</li>
<li>
<p>They also use a temperature parameter <code>τ</code>(tau) to control
uncertainty/entropy/randomness(whatever you want to call it) of the hidden state while
sampling from the PDF.</p>
<br>
<p align="center">
<img src="images/world-models/rnn-arch.png"
alt="RNN architecture" width=450>
</p>
<br>
</li>
</ul>
</li>
<li>
<h4 id="controller-c"><a class="header" href="#controller-c">Controller (C)</a></h4>
<ul>
<li>Intended to be as simple and light-weight as possible, this is just a single linear layer
which maps <code>z(t)</code> and <code>h(t)</code> directly to <code>a(t)</code>. basically, <code>a(t) = W_c (z(t) + h(t)) + b_c</code>.</li>
<li>They use the Covariance-Matrix Adaptation Evolution Strategy (CMA-ES) algorithm for
optimizing the controller network C.</li>
<li>Note that using a separate and simple Contorller allows us to swap it out (for future
experimentation/work) quickly later, they suggest that it can also be swapped out for an
evolution network.</li>
</ul>
</li>
</ul>
</li>
</ul>
<br>
<p align="center">
<img src="images/world-models/model-arch.png"
alt="Model architecture" width=350>
<img src="images/world-models/pseudocode.png"
alt="Pseudocode" width=350>
</p>
<br>
<h2 id="experiments"><a class="header" href="#experiments">Experiments</a></h2>
<ul>
<li>
<h3 id="car-racing"><a class="header" href="#car-racing">Car Racing</a></h3>
<!-- Pipeline -->
<ul>
<li>They first train the VAE(V) by collecting 10k samples acting randomly in the environment
and use the usual reconstruction loss(L2) to minimize loss between the original and the
reconstructed image. (Predicts - <code>z(t)</code>)</li>
<li>They then train the RNN-MDN(M) model using the VAE(V) to predict <code>z(t+1)</code>, given the
previous hidden state of RNN.</li>
<li>They finally train the controller(C) to control 3 continuous values - steering,
acceleration and brake.</li>
<li>Train these models separately instead of training them in an end-to-end manner, which also
makes sense and is more practical.</li>
<li>The wall time for training each model was just under an hour (using 1 GPU).</li>
<li>
<h5 id="using-only-zt"><a class="header" href="#using-only-zt">Using only <code>z(t)</code></a></h5>
<ul>
<li>The basic network scores around 632 ± 251, on par w other leaderboard models and
adding hidden layers help the model to achieve a score of 788 ± 141.</li>
</ul>
</li>
<li>
<h5 id="using-zt--ht---the-full-world-model"><a class="header" href="#using-zt--ht---the-full-world-model">Using <code>z(t) + h(t)</code> - the Full World model</a></h5>
<ul>
<li>They call this the full world model, which scores around 906 ± 21.</li>
</ul>
</li>
</ul>
<br>
<p align="center">
<img src="images/world-models/training-pipeline.png"
alt="Model architecture" width=450>
</p>
<br>
</li>
<li>
<h3 id="vizdoom"><a class="header" href="#vizdoom">VizDoom</a></h3>
<ul>
<li>
<h4 id="learning-inside-a-dream"><a class="header" href="#learning-inside-a-dream">Learning inside a dream</a></h4>
<!-- TODO: change wordings -->
<ul>
<li>If the world model is a good representation of the original game then we can train the
controller(C) directly on the trained models instead of the actual environment and this is
exactly what they do here. They train the model directly on the learned world model
instead of interacting with the actual environment.</li>
<li>No explicit rewards are given here, but the cumulative reward is defined as the total
number of timesteps the agent manages to stay alive.</li>
<li>Training the controller(C) will be done directly in the latent space and not on raw
pixels, because there are no raw pixels here as the world model only works in the latent
space.</li>
<li>They train with a high <code>τ</code>(tau) and transfer the model to the actual game which
apparently performs better than the model trained with a lower <code>τ</code> value.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="misc"><a class="header" href="#misc">Misc</a></h3>
<ul>
<li>For more details on how the models are trained, the hyperparameters, etc, take a look at the
<strong>Appendix</strong> section in the paper.</li>
<li>I was told that the <strong>Related Works</strong> section is a gold mine, so you should probably take a look
at the papers and other articles linked there.</li>
</ul>
<!-- TODO -->
<!-- be more explicit about the training pipeline -->
<!-- explain more about dreams -->
<!-- like you update the RNN with the current z(t), and ? will the recon loss be wrt to the next frame -->

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
    </body>
</html>
